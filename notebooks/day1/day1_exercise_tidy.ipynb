{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 Exercise: Cleaning Messy Cafe Sales Data\n",
    "\n",
    "**Name:** _Bálint Décsi_  \n",
    "**Date:** October 8, 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Transform a messy cafe sales dataset into a tidy format, designate and validate a primary key, and create summary tables.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "**File:** `../data/day1/dirty_cafe_sales.csv`  \n",
    "**Rows:** 10,000 cafe transactions  \n",
    "**Data Dictionary:** See `../data/day1/README.md`\n",
    "\n",
    "## Deliverable\n",
    "\n",
    "This notebook should **\"Restart & Run All\"** successfully when you're done!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Data Loading\n",
    "\n",
    "### TODO 1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.435520Z",
     "iopub.status.busy": "2025-10-07T16:37:04.435255Z",
     "iopub.status.idle": "2025-10-07T16:37:04.440874Z",
     "shell.execute_reply": "2025-10-07T16:37:04.440360Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 1: Import pandas and numpy\n",
    "# Uncomment the lines below and run this cell:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.444179Z",
     "iopub.status.busy": "2025-10-07T16:37:04.443572Z",
     "iopub.status.idle": "2025-10-07T16:37:04.447293Z",
     "shell.execute_reply": "2025-10-07T16:37:04.446499Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 2: Load the data\n",
    "# Uncomment the lines below and run this cell:\n",
    "\n",
    "df = pd.read_csv('../../data/day1/dirty_cafe_sales.csv')\n",
    "# print(f\"✅ Data loaded: {len(df):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Initial Exploration\n",
    "\n",
    "Before cleaning, let's understand what we have.\n",
    "\n",
    "### TODO 3: Display basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.450201Z",
     "iopub.status.busy": "2025-10-07T16:37:04.449966Z",
     "iopub.status.idle": "2025-10-07T16:37:04.452839Z",
     "shell.execute_reply": "2025-10-07T16:37:04.452159Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 3: Display the shape of the dataframe\n",
    "# Uncomment and run:\n",
    "\n",
    "print(f\"Dataset shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.456531Z",
     "iopub.status.busy": "2025-10-07T16:37:04.456293Z",
     "iopub.status.idle": "2025-10-07T16:37:04.458954Z",
     "shell.execute_reply": "2025-10-07T16:37:04.458390Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 3 (continued): Display the first 10 rows\n",
    "# Uncomment and run:\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.461085Z",
     "iopub.status.busy": "2025-10-07T16:37:04.460923Z",
     "iopub.status.idle": "2025-10-07T16:37:04.463027Z",
     "shell.execute_reply": "2025-10-07T16:37:04.462559Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 3 (continued): Display column names and types\n",
    "# Uncomment and run:\n",
    "\n",
    "print(\"Column Types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 4: Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.464658Z",
     "iopub.status.busy": "2025-10-07T16:37:04.464511Z",
     "iopub.status.idle": "2025-10-07T16:37:04.466371Z",
     "shell.execute_reply": "2025-10-07T16:37:04.466036Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 4: Count missing values (NaN) in each column\n",
    "# Uncomment and run:\n",
    "\n",
    "print(\"Missing Values (NaN) per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 5: Check for sentinel values\n",
    "\n",
    "Look for \"ERROR\" and \"UNKNOWN\" in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.468032Z",
     "iopub.status.busy": "2025-10-07T16:37:04.467921Z",
     "iopub.status.idle": "2025-10-07T16:37:04.469625Z",
     "shell.execute_reply": "2025-10-07T16:37:04.469281Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 5: Count \"ERROR\" values in each column\n",
    "# Uncomment and run:\n",
    "\n",
    "print(\"'ERROR' values per column:\")\n",
    "print((df == 'ERROR').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.471080Z",
     "iopub.status.busy": "2025-10-07T16:37:04.470953Z",
     "iopub.status.idle": "2025-10-07T16:37:04.472678Z",
     "shell.execute_reply": "2025-10-07T16:37:04.472373Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 5 (continued): Count \"UNKNOWN\" values in each column\n",
    "# Uncomment and run:\n",
    "\n",
    "print(\"'UNKNOWN' values per column:\")\n",
    "print((df == 'UNKNOWN').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection: What Issues Did You Find?\n",
    "\n",
    "**TODO:** Write 2-3 sentences describing the data quality issues you observed.\n",
    "\n",
    "_All of the values are stores as strings, which is an undesired state. There is a peak in NaNs for `Payment MEthod` and `Location`. Moreover, both \"ERROR\" and \"UNKNOWN\" are used in all of the columns. These sentinel values should be standardized._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Is This Data Tidy?\n",
    "\n",
    "### TODO 6: Evaluate against tidy data principles\n",
    "\n",
    "**The Three Rules:**\n",
    "1. Each variable is a column\n",
    "2. Each observation is a row\n",
    "3. Each value is a cell\n",
    "\n",
    "**Questions to answer in markdown:**\n",
    "\n",
    "1. What is the unit of observation in this dataset? (What does each row represent?)\n",
    "\n",
    "_Transaction._\n",
    "\n",
    "2. Does each variable have its own column?\n",
    "\n",
    "_Yes._\n",
    "\n",
    "3. Is this dataset tidy? Why or why not?\n",
    "\n",
    "_`Total Spent` is calculated from other two columns, so it might be counted as redundancy. Moreover, it'd be even more granular by making items in transactions the unit of observation._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Identify and Validate Primary Key\n",
    "\n",
    "### TODO 7: Identify the primary key candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.474190Z",
     "iopub.status.busy": "2025-10-07T16:37:04.474081Z",
     "iopub.status.idle": "2025-10-07T16:37:04.475723Z",
     "shell.execute_reply": "2025-10-07T16:37:04.475465Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 7: Check if 'Transaction ID' is unique\n",
    "# Uncomment and run:\n",
    "\n",
    "is_unique = df['Transaction ID'].is_unique\n",
    "print(f\"Is 'Transaction ID' unique? {is_unique}\")\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Unique Transaction IDs: {df['Transaction ID'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.477241Z",
     "iopub.status.busy": "2025-10-07T16:37:04.477138Z",
     "iopub.status.idle": "2025-10-07T16:37:04.478723Z",
     "shell.execute_reply": "2025-10-07T16:37:04.478471Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 7 (continued): Check for any NULL values in 'Transaction ID'\n",
    "# Uncomment and run:\n",
    "\n",
    "null_count = df['Transaction ID'].isnull().sum()\n",
    "print(f\"NULL Transaction IDs: {null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.479870Z",
     "iopub.status.busy": "2025-10-07T16:37:04.479799Z",
     "iopub.status.idle": "2025-10-07T16:37:04.481282Z",
     "shell.execute_reply": "2025-10-07T16:37:04.481050Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 7 (continued): If there are duplicates, find them\n",
    "# Uncomment and run:\n",
    "\n",
    "duplicates = df[df.duplicated(subset=['Transaction ID'], keep=False)]\n",
    "print(f\"Duplicate rows: {len(duplicates)}\")\n",
    "if len(duplicates) > 0:\n",
    "    print(\"\\nShowing first few duplicates:\")\n",
    "    display(duplicates.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 8: Write validation assertions\n",
    "\n",
    "Once you've confirmed (or fixed) the primary key, write assertions to prove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.482606Z",
     "iopub.status.busy": "2025-10-07T16:37:04.482536Z",
     "iopub.status.idle": "2025-10-07T16:37:04.483860Z",
     "shell.execute_reply": "2025-10-07T16:37:04.483629Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 8: Add assertions to validate primary key\n",
    "# Uncomment and run (these will error if checks fail):\n",
    "\n",
    "assert df['Transaction ID'].is_unique, \"❌ Duplicate transaction IDs found\"\n",
    "assert df['Transaction ID'].notna().all(), \"❌ NULL transaction IDs found\"\n",
    "print(\"✅ Transaction ID is a valid primary key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection: Primary Key\n",
    "\n",
    "**TODO:** Explain what you found and any decisions you made.\n",
    "\n",
    "_Transaction ID a good primary key. I haven't found any issues. It is unique and non-NULL._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Handle Missing Values\n",
    "\n",
    "### TODO 9: Standardize missing value representations\n",
    "\n",
    "Convert \"ERROR\", \"UNKNOWN\", and empty strings to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.485071Z",
     "iopub.status.busy": "2025-10-07T16:37:04.485005Z",
     "iopub.status.idle": "2025-10-07T16:37:04.486405Z",
     "shell.execute_reply": "2025-10-07T16:37:04.486172Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 9: Replace sentinel values with NaN\n",
    "# Uncomment and run:\n",
    "\n",
    "df = df.replace(['ERROR', 'UNKNOWN', ''], np.nan)\n",
    "print(\"✅ Replaced 'ERROR', 'UNKNOWN', and empty strings with NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.487547Z",
     "iopub.status.busy": "2025-10-07T16:37:04.487451Z",
     "iopub.status.idle": "2025-10-07T16:37:04.488900Z",
     "shell.execute_reply": "2025-10-07T16:37:04.488670Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 9 (continued): Check missing values again after standardization\n",
    "# Uncomment and run:\n",
    "\n",
    "print(\"Missing values after standardization:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal missing values: {df.isnull().sum().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 10: Decide how to handle missing values\n",
    "\n",
    "**Options:**\n",
    "- Drop rows with missing values in critical columns\n",
    "- Fill with default values\n",
    "- Keep as NaN (document impact on analysis)\n",
    "\n",
    "**Your strategy:**\n",
    "\n",
    "_[Write your strategy here. Example: \"I will keep NULL for `Payment Method` and `Location` because it represents around 30% of rows. ALthough, I decided to exclude NULLs from `Total Spent` as it is a crucial feature.\"]_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.490083Z",
     "iopub.status.busy": "2025-10-07T16:37:04.490010Z",
     "iopub.status.idle": "2025-10-07T16:37:04.491558Z",
     "shell.execute_reply": "2025-10-07T16:37:04.491304Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 10: Implement your missing value strategy\n",
    "# This is a decision point - choose your approach!\n",
    "# Below is ONE option: Keep NaN as-is (document in reflection above)\n",
    "# Uncomment and run:\n",
    "\n",
    "# For this exercise, we'll keep NaN values and handle them in analysis\n",
    "# (You could also drop rows or fill values - document your choice above!)\n",
    "print(\"✅ Missing value strategy: excluding NULLs from `Total Spent` as it is a crucial feature.\")\n",
    "df = df.dropna(subset=['Total Spent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "print(\"Missing values after standardization:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal missing values: {df.isnull().sum().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Fix Type Issues\n",
    "\n",
    "### TODO 11: Convert Quantity to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.492663Z",
     "iopub.status.busy": "2025-10-07T16:37:04.492601Z",
     "iopub.status.idle": "2025-10-07T16:37:04.493920Z",
     "shell.execute_reply": "2025-10-07T16:37:04.493703Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 11: Convert Quantity to integer\n",
    "# Uncomment and run:\n",
    "\n",
    "df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce').astype('Int64')\n",
    "print(\"✅ Quantity converted to Int64 (allows NaN)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 12: Convert prices to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.494959Z",
     "iopub.status.busy": "2025-10-07T16:37:04.494897Z",
     "iopub.status.idle": "2025-10-07T16:37:04.496231Z",
     "shell.execute_reply": "2025-10-07T16:37:04.496018Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 12: Convert 'Price Per Unit' to float\n",
    "# Uncomment and run:\n",
    "\n",
    "df['Price Per Unit'] = pd.to_numeric(df['Price Per Unit'], errors='coerce')\n",
    "print(\"✅ 'Price Per Unit' converted to float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.497301Z",
     "iopub.status.busy": "2025-10-07T16:37:04.497222Z",
     "iopub.status.idle": "2025-10-07T16:37:04.498557Z",
     "shell.execute_reply": "2025-10-07T16:37:04.498360Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 12 (continued): Convert 'Total Spent' to float\n",
    "# Uncomment and run:\n",
    "\n",
    "df['Total Spent'] = pd.to_numeric(df['Total Spent'], errors='coerce')\n",
    "print(\"✅ 'Total Spent' converted to float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 13: Convert Transaction Date to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.499594Z",
     "iopub.status.busy": "2025-10-07T16:37:04.499535Z",
     "iopub.status.idle": "2025-10-07T16:37:04.500901Z",
     "shell.execute_reply": "2025-10-07T16:37:04.500651Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 13: Parse Transaction Date as datetime\n",
    "# Uncomment and run:\n",
    "\n",
    "df['Transaction Date'] = pd.to_datetime(df['Transaction Date'], errors='coerce')\n",
    "print(\"✅ 'Transaction Date' converted to datetime64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 14: Verify types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.501979Z",
     "iopub.status.busy": "2025-10-07T16:37:04.501917Z",
     "iopub.status.idle": "2025-10-07T16:37:04.503220Z",
     "shell.execute_reply": "2025-10-07T16:37:04.503004Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 14: Display dtypes to verify conversions worked\n",
    "# Uncomment and run:\n",
    "\n",
    "print(\"Updated Column Types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 15: Write type assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.504248Z",
     "iopub.status.busy": "2025-10-07T16:37:04.504167Z",
     "iopub.status.idle": "2025-10-07T16:37:04.505547Z",
     "shell.execute_reply": "2025-10-07T16:37:04.505348Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 15: Add assertions to validate types\n",
    "# Uncomment and run:\n",
    "\n",
    "assert df['Quantity'].dtype in ['int64', 'Int64'], \"❌ Quantity should be integer\"\n",
    "assert df['Price Per Unit'].dtype == 'float64', \"❌ Price should be float\"\n",
    "assert df['Transaction Date'].dtype == 'datetime64[ns]', \"❌ Date should be datetime\"\n",
    "print(\"✅ All types are correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Validate Data Integrity\n",
    "\n",
    "### TODO 16: Check if Total Spent = Quantity × Price Per Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.506508Z",
     "iopub.status.busy": "2025-10-07T16:37:04.506448Z",
     "iopub.status.idle": "2025-10-07T16:37:04.507755Z",
     "shell.execute_reply": "2025-10-07T16:37:04.507531Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 16: Calculate expected total\n",
    "# Uncomment and run:\n",
    "\n",
    "df['Calculated Total'] = df['Quantity'] * df['Price Per Unit']\n",
    "print(\"✅ Calculated expected totals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.508681Z",
     "iopub.status.busy": "2025-10-07T16:37:04.508625Z",
     "iopub.status.idle": "2025-10-07T16:37:04.509965Z",
     "shell.execute_reply": "2025-10-07T16:37:04.509757Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 16 (continued): Compare with actual Total Spent\n",
    "# This uses np.isclose() for float comparison (allows tiny rounding differences)\n",
    "# Uncomment and run:\n",
    "\n",
    "mask = df['Total Spent'].notna() & df['Calculated Total'].notna()\n",
    "mismatches = ~np.isclose(\n",
    "    df.loc[mask, 'Total Spent'], \n",
    "    df.loc[mask, 'Calculated Total'],\n",
    "    rtol=1e-05  # Relative tolerance for floating point comparison\n",
    ")\n",
    "print(f\"Mismatches found: {mismatches.sum()} out of {mask.sum()} rows with data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 17: Check for impossible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.511001Z",
     "iopub.status.busy": "2025-10-07T16:37:04.510919Z",
     "iopub.status.idle": "2025-10-07T16:37:04.512263Z",
     "shell.execute_reply": "2025-10-07T16:37:04.512065Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 17: Check for negative or zero prices\n",
    "# Uncomment and run:\n",
    "\n",
    "bad_prices = df[df['Price Per Unit'] <= 0]\n",
    "print(f\"Rows with price <= 0: {len(bad_prices)}\")\n",
    "if len(bad_prices) > 0:\n",
    "    display(bad_prices[['Transaction ID', 'Item', 'Price Per Unit']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.513257Z",
     "iopub.status.busy": "2025-10-07T16:37:04.513196Z",
     "iopub.status.idle": "2025-10-07T16:37:04.514548Z",
     "shell.execute_reply": "2025-10-07T16:37:04.514318Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 17 (continued): Check for zero or negative quantities\n",
    "# Uncomment and run:\n",
    "\n",
    "bad_qty = df[df['Quantity'] <= 0]\n",
    "print(f\"Rows with quantity <= 0: {len(bad_qty)}\")\n",
    "if len(bad_qty) > 0:\n",
    "    display(bad_qty[['Transaction ID', 'Item', 'Quantity']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection: Data Integrity\n",
    "\n",
    "**TODO:** What did you find? How did you handle integrity issues?\n",
    "\n",
    "_I haven't found any integrity issue._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Create Summary Tables\n",
    "\n",
    "Now that data is clean, answer some business questions!\n",
    "\n",
    "### TODO 18: Total sales by payment method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.515741Z",
     "iopub.status.busy": "2025-10-07T16:37:04.515646Z",
     "iopub.status.idle": "2025-10-07T16:37:04.517504Z",
     "shell.execute_reply": "2025-10-07T16:37:04.517044Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 18: Calculate total revenue and transaction count by payment method\n",
    "# Uncomment and run (this one is fully worked as an example):\n",
    "\n",
    "payment_summary = df.groupby('Payment Method').agg({\n",
    "    'Total Spent': 'sum',\n",
    "    'Transaction ID': 'count'\n",
    "}).round(2)\n",
    "\n",
    "payment_summary.columns = ['Total Revenue', 'Transaction Count']\n",
    "payment_summary = payment_summary.sort_values('Total Revenue', ascending=False)\n",
    "\n",
    "print(\"Sales by Payment Method:\")\n",
    "display(payment_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 19: Most popular items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.518528Z",
     "iopub.status.busy": "2025-10-07T16:37:04.518469Z",
     "iopub.status.idle": "2025-10-07T16:37:04.519791Z",
     "shell.execute_reply": "2025-10-07T16:37:04.519573Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 19: Find most popular items by quantity sold\n",
    "# Pattern: df.groupby('Column')['Metric'].sum().sort_values(ascending=False)\n",
    "# Uncomment and adapt:\n",
    "\n",
    "popular_items = df.groupby('Item')['Quantity'].sum().sort_values(ascending=False)\n",
    "print(\"Most Popular Items (by quantity):\")\n",
    "display(popular_items.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.520813Z",
     "iopub.status.busy": "2025-10-07T16:37:04.520733Z",
     "iopub.status.idle": "2025-10-07T16:37:04.522103Z",
     "shell.execute_reply": "2025-10-07T16:37:04.521893Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 19 (continued): Find highest revenue items\n",
    "# Use the same pattern but with 'Total Spent' instead of 'Quantity'\n",
    "# Uncomment and adapt:\n",
    "\n",
    "revenue_items = df.groupby('Item')['Total Spent'].sum().sort_values(ascending=False).round(2)\n",
    "print(\"Highest Revenue Items:\")\n",
    "display(revenue_items.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 20: Location comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.523044Z",
     "iopub.status.busy": "2025-10-07T16:37:04.522992Z",
     "iopub.status.idle": "2025-10-07T16:37:04.524334Z",
     "shell.execute_reply": "2025-10-07T16:37:04.524116Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 20: Compare transaction volume and average transaction value by location\n",
    "# This uses .agg() with multiple functions (like TODO 18)\n",
    "# Uncomment and run:\n",
    "\n",
    "location_summary = df.groupby('Location').agg({\n",
    "    'Transaction ID': 'count',\n",
    "    'Total Spent': ['sum', 'mean']\n",
    "}).round(2)\n",
    "\n",
    "location_summary.columns = ['Transaction Count', 'Total Revenue', 'Avg Transaction Value']\n",
    "print(\"Sales by Location:\")\n",
    "display(location_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 9: Final Validation\n",
    "\n",
    "### TODO 21: Run all validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T16:37:04.525312Z",
     "iopub.status.busy": "2025-10-07T16:37:04.525260Z",
     "iopub.status.idle": "2025-10-07T16:37:04.526675Z",
     "shell.execute_reply": "2025-10-07T16:37:04.526476Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 21: Gather all your assertions in one cell to prove data quality\n",
    "# Uncomment and run:\n",
    "\n",
    "print(\"Running final validation...\\n\")\n",
    "\n",
    "# Primary key\n",
    "assert df['Transaction ID'].is_unique, \"❌ Duplicate transaction IDs\"\n",
    "assert df['Transaction ID'].notna().all(), \"❌ NULL transaction IDs\"\n",
    "print(\"✅ Primary key validated\")\n",
    "\n",
    "# Types\n",
    "assert df['Quantity'].dtype in ['int64', 'Int64'], \"❌ Quantity type wrong\"\n",
    "assert df['Price Per Unit'].dtype == 'float64', \"❌ Price type wrong\"\n",
    "assert df['Transaction Date'].dtype == 'datetime64[ns]', \"❌ Date type wrong\"\n",
    "print(\"✅ Types validated\")\n",
    "\n",
    "# Data ranges (only check non-null values)\n",
    "assert (df['Quantity'].dropna() > 0).all(), \"❌ Invalid quantities found\"\n",
    "assert (df['Price Per Unit'].dropna() > 0).all(), \"❌ Invalid prices found\"\n",
    "print(\"✅ Data ranges validated\")\n",
    "\n",
    "print(\"\\n✅ All validations passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 10: Documentation\n",
    "\n",
    "### TODO 22: Document your data cleaning process\n",
    "\n",
    "Write a brief summary (8-10 sentences) of:\n",
    "1. What problems you found\n",
    "2. What decisions you made\n",
    "3. What the implications are for analysis\n",
    "4. What a stakeholder should know about this data\n",
    "\n",
    "---\n",
    "\n",
    "## Data Cleaning Summary\n",
    "\n",
    "_This is a dataset of store transactions with unique IDs for each observation._\n",
    "\n",
    "### Issues Found\n",
    "- _data types: everything is stored in strings_,\n",
    "- _missing data: for `Payment MEthod` and `Location` almost 30%_,\n",
    "- _sentinel values are used, e. g. \"UNKNOWN\".\n",
    "\n",
    "### Actions Taken\n",
    "- _convert columns to integer, float and datetime types_,\n",
    "- _not dropping all the rows in the 30%_,\n",
    "- _standardize with \"NaN\"_.\n",
    "\n",
    "### Assumptions Made\n",
    "- _since both columns mentioned above with high percentage of NULLs have a quite uniform distribution, there might be something behind the missing values_,\n",
    "- _other columns seems like having valid data, which has been checked with type conversion assertions_,\n",
    "- _should ask data collection team if there's difference between \"ERROR\" and \"UNKNOWN\"_. \n",
    "\n",
    "### Implications for Analysis\n",
    "- _be aware that although I haven't dropped missing value rows, I neither imputated them with any value_,\n",
    "- _I strongly suggest finding (a) good candidate value(s) for usage there_.\n",
    "\n",
    "### Data Quality Assessment\n",
    "- _overall, now 100% of data is usable with the constraint of the two columns \"NaN\" value consideration in further analysis_.\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've successfully cleaned a real messy dataset using tidy data principles!\n",
    "\n",
    "**Final check:** Can you **\"Restart & Run All\"** successfully? That's the gold standard!\n",
    "\n",
    "**Reflection:** What was the hardest part? What did you learn?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECBS5294",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
